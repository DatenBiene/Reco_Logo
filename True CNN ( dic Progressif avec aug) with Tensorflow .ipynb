{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressif avec data aug : 77.3% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_aug=8\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "\n",
    "imPath = \"Database/All_pictures/\"\n",
    "ls_path = glob(os.path.join(imPath, '*' ))\n",
    "\n",
    "images_base_vide=[]\n",
    "label_nom_vide=[]\n",
    "\n",
    "for file in ls_path: \n",
    "    im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "    images_base_vide+=[im]\n",
    "    label_nom_vide+=[file.split('\\\\')[1].split('_')[0]]   ## Attention ici Solène si ça marche pas . Pas meme code pour chemin ...\n",
    "\n",
    "images_base_raw=np.array(images_base_vide)\n",
    "label_nom_raw=np.array(label_nom_vide)\n",
    "\n",
    "def print_exemple_image(num_image,X=images_base_raw,y=label_nom_vide) :\n",
    "    plt.imshow(X[num_image],cmap='Greys')\n",
    "    plt.suptitle(\"Image n°\"+str(num_image)+\" : \"+str(y[num_image]), fontsize=20)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ramdom_seed_fixée=5\n",
    "\n",
    "def get_split_classique() :\n",
    "    return train_test_split(images_base_raw, label_nom_raw, test_size=0.20, random_state=ramdom_seed_fixée)\n",
    "\n",
    "X_train_classique , X_test_classique , Y_train_classique , Y_test_classique = get_split_classique()\n",
    "\n",
    "label_to_OneHot = {'Deezer':[1,0,0,0],'Messenger':[0,1,0,0],'Facebook':[0,0,1,0],'Tinder':[0,0,0,1]}\n",
    "label_to_num = {'Deezer':0 ,'Messenger':1 ,'Facebook':2,'Tinder':3} \n",
    "num_to_label={0:'Deezer',1:'Messenger' ,2:'Facebook',3:'Tinder'}\n",
    "\n",
    "def transformation_dictionnaire_image(X,Y,data_aug=1,num_pixel_cote=64) : \n",
    "    taille = X.shape[0]\n",
    "    data_base={'image':[], 'data': [], 'label_num' : [],'label_OneHot' : [],'nom_label' :[]}\n",
    "    for i in range(taille) : \n",
    "        im = cv2.resize(X[i], (num_pixel_cote,num_pixel_cote))\n",
    "        name=Y[i]\n",
    "        \n",
    "        for k in range(data_aug):\n",
    "            num_rows, num_cols = im.shape[:2]\n",
    "            rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), k*5, 1)\n",
    "            im_rotation = cv2.warpAffine(im, rotation_matrix, (num_cols, num_rows))\n",
    "        \n",
    "            \n",
    "            data_base['image']+=[im_rotation]\n",
    "            data_base['data']+=[np.ndarray.flatten(im_rotation)]\n",
    "            data_base['label_num']+=[label_to_num[name]]\n",
    "            data_base['label_OneHot']+=[label_to_OneHot[name]]\n",
    "            data_base['nom_label']+=[name]\n",
    "            \n",
    "            \n",
    "    data_base['image']=np.array( data_base['image'])\n",
    "    data_base['data']=np.array(data_base['data'])\n",
    "    data_base['label_num']=np.array(data_base['label_num'])\n",
    "    data_base['label_OneHot']=np.array(data_base['label_OneHot'])\n",
    "    data_base['nom_label']=np.array(data_base['nom_label'])\n",
    "    \n",
    "    return data_base\n",
    "        \n",
    "Train_Classique = transformation_dictionnaire_image(X_train_classique,Y_train_classique,data_aug=num_aug)\n",
    "Test_Classique = transformation_dictionnaire_image(X_test_classique,Y_test_classique)\n",
    "\n",
    "catégories= ['Deezer','Facebook','Messenger','Tinder']\n",
    "\n",
    "X_train_progressif , X_test_progressif , Y_train_progressif , Y_test_progressif = [],[],[],[]\n",
    "\n",
    "for cat in catégories : \n",
    "    imPath = \"Database/\"+cat+\"/\"\n",
    "    ls_path = glob(os.path.join(imPath, '*' ))\n",
    "\n",
    "    taille_train=len(ls_path)-len(ls_path)//5\n",
    "    #Train\n",
    "    for file in ls_path[:taille_train]: \n",
    "        im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "        X_train_progressif+=[im]\n",
    "        Y_train_progressif+=[file.split('\\\\')[1].split('_')[0]]\n",
    "\n",
    "    #Test\n",
    "    for file in ls_path[taille_train:]: \n",
    "        im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "        X_test_progressif+=[im]\n",
    "        Y_test_progressif+=[file.split('\\\\')[1].split('_')[0]]\n",
    "    \n",
    "\n",
    "X_train_progressif=np.array(X_train_progressif)\n",
    "Y_train_progressif=np.array(Y_train_progressif)\n",
    "X_test_progressif=np.array(X_test_progressif)\n",
    "Y_test_progressif=np.array(Y_test_progressif)\n",
    "\n",
    "\n",
    "Train_Progressif = transformation_dictionnaire_image(X_train_progressif,Y_train_progressif,data_aug=num_aug)\n",
    "Test_Progressif = transformation_dictionnaire_image(X_test_progressif,Y_test_progressif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_dic=Train_Progressif\n",
    "Test_dic=Test_Progressif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t9688\n",
      "- Test-set:\t\t300\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(Train_dic['image'].shape[0]))\n",
    "print(\"- Test-set:\\t\\t{}\".format(Test_dic['image'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of pixels in each dimension of an image.\n",
    "img_size = Train_dic['image'].shape[1]\n",
    "\n",
    "# The images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = Train_dic['data'].shape[1]\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = Train_dic['image'].shape[1:]\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 4 \n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3,  figsize=(10,5))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEvCAYAAACdahL0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niYiILMAATEREZIH/B+eFW1xmsVUyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x255597aa4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = Train_dic['image'][0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = [num_to_label[i] for i in Train_dic['label_num'][0:9]]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 32, 32, 16) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 16, 16, 36) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 9216) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-c6320b202c3b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_batch(X,y,batch_size) :\n",
    "    index=random.sample(range(1,y.shape[0]), batch_size)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "batch_size=100\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = get_random_batch(Train_dic['data'],Train_dic['label_OneHot'],batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_example_errors():\n",
    "    # Use TensorFlow to get a list of boolean values\n",
    "    # whether each test-image has been correctly classified,\n",
    "    # and a list for the predicted class of each image.\n",
    "    correct, cls_pred = session.run([correct_prediction, y_pred_cls],\n",
    "                                    feed_dict=feed_dict_test)\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = Test_dic['data'][incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = Test_dic['label_num'][incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=[num_to_label[i] for i in cls_true[0:9]],\n",
    "                cls_pred=[num_to_label[i] for i in cls_pred[0:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = Test_dic['label_num']\n",
    "\n",
    "    # Get the predicted classifications for the test-set.\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, [num_to_label[i] for i in range(num_classes)])\n",
    "    plt.yticks(tick_marks, [num_to_label[i] for i in range(num_classes)])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_test = {x: Test_dic['data'],\n",
    "                  y_true: Test_dic['label_OneHot'],\n",
    "                  y_true_cls: Test_dic['label_num']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 30.0%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:  24.0%\n",
      "Time usage: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 29.7%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    101, Training Accuracy:  61.0%\n",
      "Time usage: 0:01:52\n",
      "Accuracy on test-set: 61.3%\n",
      "Optimization Iteration:    201, Training Accuracy:  81.0%\n",
      "Time usage: 0:01:50\n",
      "Accuracy on test-set: 65.3%\n",
      "Optimization Iteration:    301, Training Accuracy:  77.0%\n",
      "Time usage: 0:01:54\n",
      "Accuracy on test-set: 68.0%\n",
      "Optimization Iteration:    401, Training Accuracy:  92.0%\n",
      "Time usage: 0:01:51\n",
      "Accuracy on test-set: 69.3%\n",
      "Optimization Iteration:    501, Training Accuracy:  97.0%\n",
      "Time usage: 0:01:49\n",
      "Accuracy on test-set: 73.7%\n",
      "Optimization Iteration:    601, Training Accuracy:  98.0%\n",
      "Time usage: 0:01:51\n",
      "Accuracy on test-set: 73.0%\n",
      "Optimization Iteration:    701, Training Accuracy: 100.0%\n",
      "Time usage: 0:01:53\n",
      "Accuracy on test-set: 76.0%\n",
      "Optimization Iteration:    801, Training Accuracy:  97.0%\n",
      "Time usage: 0:01:51\n",
      "Accuracy on test-set: 76.0%\n",
      "Optimization Iteration:    901, Training Accuracy:  97.0%\n",
      "Time usage: 0:01:51\n",
      "Accuracy on test-set: 76.7%\n",
      "Optimization Iteration:   1001, Training Accuracy:  98.0%\n",
      "Time usage: 0:01:53\n",
      "Accuracy on test-set: 77.3%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): \n",
    "    optimize(num_iterations=100) \n",
    "    print_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
