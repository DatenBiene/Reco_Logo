{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressif sans data aug : 76.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_aug=1\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "\n",
    "imPath = \"Database/All_pictures/\"\n",
    "ls_path = glob(os.path.join(imPath, '*' ))\n",
    "\n",
    "images_base_vide=[]\n",
    "label_nom_vide=[]\n",
    "\n",
    "for file in ls_path: \n",
    "    im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "    images_base_vide+=[im]\n",
    "    label_nom_vide+=[file.split('\\\\')[1].split('_')[0]]   ## Attention ici Solène si ça marche pas . Pas meme code pour chemin ...\n",
    "\n",
    "images_base_raw=np.array(images_base_vide)\n",
    "label_nom_raw=np.array(label_nom_vide)\n",
    "\n",
    "def print_exemple_image(num_image,X=images_base_raw,y=label_nom_vide) :\n",
    "    plt.imshow(X[num_image],cmap='Greys')\n",
    "    plt.suptitle(\"Image n°\"+str(num_image)+\" : \"+str(y[num_image]), fontsize=20)\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ramdom_seed_fixée=5\n",
    "\n",
    "def get_split_classique() :\n",
    "    return train_test_split(images_base_raw, label_nom_raw, test_size=0.20, random_state=ramdom_seed_fixée)\n",
    "\n",
    "X_train_classique , X_test_classique , Y_train_classique , Y_test_classique = get_split_classique()\n",
    "\n",
    "label_to_OneHot = {'Deezer':[1,0,0,0],'Messenger':[0,1,0,0],'Facebook':[0,0,1,0],'Tinder':[0,0,0,1]}\n",
    "label_to_num = {'Deezer':0 ,'Messenger':1 ,'Facebook':2,'Tinder':3} \n",
    "num_to_label={0:'Deezer',1:'Messenger' ,2:'Facebook',3:'Tinder'}\n",
    "\n",
    "def transformation_dictionnaire_image(X,Y,data_aug=1,num_pixel_cote=64) : \n",
    "    taille = X.shape[0]\n",
    "    data_base={'image':[], 'data': [], 'label_num' : [],'label_OneHot' : [],'nom_label' :[]}\n",
    "    for i in range(taille) : \n",
    "        im = cv2.resize(X[i], (num_pixel_cote,num_pixel_cote))\n",
    "        name=Y[i]\n",
    "        \n",
    "        for k in range(data_aug):\n",
    "            num_rows, num_cols = im.shape[:2]\n",
    "            rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), k*5, 1)\n",
    "            im_rotation = cv2.warpAffine(im, rotation_matrix, (num_cols, num_rows))\n",
    "        \n",
    "            \n",
    "            data_base['image']+=[im_rotation]\n",
    "            data_base['data']+=[np.ndarray.flatten(im_rotation)]\n",
    "            data_base['label_num']+=[label_to_num[name]]\n",
    "            data_base['label_OneHot']+=[label_to_OneHot[name]]\n",
    "            data_base['nom_label']+=[name]\n",
    "            \n",
    "            \n",
    "    data_base['image']=np.array( data_base['image'])\n",
    "    data_base['data']=np.array(data_base['data'])\n",
    "    data_base['label_num']=np.array(data_base['label_num'])\n",
    "    data_base['label_OneHot']=np.array(data_base['label_OneHot'])\n",
    "    data_base['nom_label']=np.array(data_base['nom_label'])\n",
    "    \n",
    "    return data_base\n",
    "        \n",
    "Train_Classique = transformation_dictionnaire_image(X_train_classique,Y_train_classique,data_aug=num_aug)\n",
    "Test_Classique = transformation_dictionnaire_image(X_test_classique,Y_test_classique)\n",
    "\n",
    "catégories= ['Deezer','Facebook','Messenger','Tinder']\n",
    "\n",
    "X_train_progressif , X_test_progressif , Y_train_progressif , Y_test_progressif = [],[],[],[]\n",
    "\n",
    "for cat in catégories : \n",
    "    imPath = \"Database/\"+cat+\"/\"\n",
    "    ls_path = glob(os.path.join(imPath, '*' ))\n",
    "\n",
    "    taille_train=len(ls_path)-len(ls_path)//5\n",
    "    #Train\n",
    "    for file in ls_path[:taille_train]: \n",
    "        im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "        X_train_progressif+=[im]\n",
    "        Y_train_progressif+=[file.split('\\\\')[1].split('_')[0]]\n",
    "\n",
    "    #Test\n",
    "    for file in ls_path[taille_train:]: \n",
    "        im = np.array(Image.open(file))[:,:,3]   #On charge l'image\n",
    "        X_test_progressif+=[im]\n",
    "        Y_test_progressif+=[file.split('\\\\')[1].split('_')[0]]\n",
    "    \n",
    "\n",
    "X_train_progressif=np.array(X_train_progressif)\n",
    "Y_train_progressif=np.array(Y_train_progressif)\n",
    "X_test_progressif=np.array(X_test_progressif)\n",
    "Y_test_progressif=np.array(Y_test_progressif)\n",
    "\n",
    "\n",
    "Train_Progressif = transformation_dictionnaire_image(X_train_progressif,Y_train_progressif,data_aug=num_aug)\n",
    "Test_Progressif = transformation_dictionnaire_image(X_test_progressif,Y_test_progressif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_dic=Train_Progressif\n",
    "Test_dic=Test_Progressif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t1211\n",
      "- Test-set:\t\t300\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(Train_dic['image'].shape[0]))\n",
    "print(\"- Test-set:\\t\\t{}\".format(Test_dic['image'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of pixels in each dimension of an image.\n",
    "img_size = Train_dic['image'].shape[1]\n",
    "\n",
    "# The images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = Train_dic['data'].shape[1]\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = Train_dic['image'].shape[1:]\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 4 \n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3,  figsize=(10,5))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAEvCAYAAACdahL0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAiYiIrLA/wMEmgXtBWYM8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bbccde04e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = Train_dic['image'][0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = [num_to_label[i] for i in Train_dic['label_num'][0:9]]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 32, 32, 16) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv2, weights_conv2 = \\\n",
    "    new_conv_layer(input=layer_conv1,\n",
    "                   num_input_channels=num_filters1,\n",
    "                   filter_size=filter_size2,\n",
    "                   num_filters=num_filters2,\n",
    "                   use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 16, 16, 36) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 9216) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(layer_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-c6320b202c3b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_batch(X,y,batch_size) :\n",
    "    index=random.sample(range(1,y.shape[0]), batch_size)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "batch_size=100\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = get_random_batch(Train_dic['data'],Train_dic['label_OneHot'],batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_example_errors():\n",
    "    # Use TensorFlow to get a list of boolean values\n",
    "    # whether each test-image has been correctly classified,\n",
    "    # and a list for the predicted class of each image.\n",
    "    correct, cls_pred = session.run([correct_prediction, y_pred_cls],\n",
    "                                    feed_dict=feed_dict_test)\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = Test_dic['data'][incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = Test_dic['label_num'][incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=[num_to_label[i] for i in cls_true[0:9]],\n",
    "                cls_pred=[num_to_label[i] for i in cls_pred[0:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = Test_dic['label_num']\n",
    "\n",
    "    # Get the predicted classifications for the test-set.\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, [num_to_label[i] for i in range(num_classes)])\n",
    "    plt.yticks(tick_marks, [num_to_label[i] for i in range(num_classes)])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_test = {x: Test_dic['data'],\n",
    "                  y_true: Test_dic['label_OneHot'],\n",
    "                  y_true_cls: Test_dic['label_num']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 27.7%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:  23.0%\n",
      "Time usage: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 26.3%\n"
     ]
    }
   ],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    101, Training Accuracy:  89.0%\n",
      "Time usage: 0:02:14\n",
      "Accuracy on test-set: 68.3%\n",
      "Optimization Iteration:    201, Training Accuracy:  99.0%\n",
      "Time usage: 0:01:56\n",
      "Accuracy on test-set: 72.3%\n",
      "Optimization Iteration:    301, Training Accuracy:  99.0%\n",
      "Time usage: 0:01:55\n",
      "Accuracy on test-set: 73.3%\n",
      "Optimization Iteration:    401, Training Accuracy:  99.0%\n",
      "Time usage: 0:01:53\n",
      "Accuracy on test-set: 74.3%\n",
      "Optimization Iteration:    501, Training Accuracy:  99.0%\n",
      "Time usage: 0:01:50\n",
      "Accuracy on test-set: 76.3%\n",
      "Optimization Iteration:    601, Training Accuracy: 100.0%\n",
      "Time usage: 0:01:52\n",
      "Accuracy on test-set: 74.0%\n",
      "Optimization Iteration:    701, Training Accuracy: 100.0%\n",
      "Time usage: 0:01:51\n",
      "Accuracy on test-set: 75.7%\n",
      "Optimization Iteration:    801, Training Accuracy:  96.0%\n",
      "Time usage: 0:01:54\n",
      "Accuracy on test-set: 75.0%\n",
      "Optimization Iteration:    901, Training Accuracy: 100.0%\n",
      "Time usage: 0:01:54\n",
      "Accuracy on test-set: 76.3%\n",
      "Optimization Iteration:   1001, Training Accuracy: 100.0%\n",
      "Time usage: 0:01:53\n",
      "Accuracy on test-set: 76.7%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): \n",
    "    optimize(num_iterations=100) \n",
    "    print_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
